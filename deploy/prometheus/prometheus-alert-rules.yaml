---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prometheus
    role: alert-rules
  name: prometheus-clickhouse-operator-rules
spec:
  groups:
    - name: ./clickhouse-operator.rules
      rules:
        - alert: ClickHouseMetricsExporterDown
          expr: up{job='clickhouse-operator-metrics'} == 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.pod }}"
            summary: "metrics-exporter possible down"
            description: "`{{ $labels.pod }}` in namespace `{{ $labels.namespace }}` not sent data more than 1 minutes. Please check instance status via ```kubectl logs -f -n {{ $labels.namespace }} {{ $labels.pod }} -c metrics-exporter```"

        - alert: ClickHouseServerDown
          expr: chi_clickhouse_metric_fetch_errors > 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "clickhouse-server possible down"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` failed metrics fetch `{{ $labels.fetch_type }}`.
              Please check instance status
              ```kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1)```

        - alert: ClickHouseServerRestartRecently
          expr: chi_clickhouse_metric_Uptime > 1 < 180
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "clickhouse-server started recently"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` has been start less than 3 minutes ago.

        - alert: ClickHouseDNSErrors
          expr: increase(chi_clickhouse_event_DNSError[1m]) > 0 or increase(chi_clickhouse_event_NetworkErrors[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "DNS errors occurred"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}`
              Please check DNS settings and remote_servers part of /etc/clickhouse-server/
              See documentation:
              - https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-remote-servers
              - https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-disable-internal-dns-cache
              - https://clickhouse.tech/docs/en/query_language/system/#query_language-system-drop-dns-cache

        - alert: ClickHouseDistributedFilesToInsertHigh
          expr: chi_clickhouse_metric_DistributedFilesToInsert > 50
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "clickhouse-server have Distributed Files to Insert > 50"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have too much files which not insert to `*MergeTree` tables via `Distributed` table engine
              https://clickhouse.tech/docs/en/operations/table_engines/distributed/
              When you insert data to `Distributed` table.
              Data is written to target `*MergreTree` tables asynchronously.
              When inserted in the table, the data block is just written to the local file system.
              The data is sent to the remote servers in the background as soon as possible.
              The period for sending data is managed by the `distributed_directory_monitor_sleep_time_ms` and `distributed_directory_monitor_max_sleep_time_ms` settings.
              The Distributed engine sends each file with inserted data separately, but you can enable batch sending of files with the `distributed_directory_monitor_batch_inserts` setting

              Also, you can manage distributed tables
              https://clickhouse.tech/docs/en/sql-reference/statements/system/#query-language-system-distributed

        - alert: ClickHouseDistributedConnectionExceptions
          expr: increase(chi_clickhouse_event_DistributedConnectionFailTry[1m]) > 0 or increase(chi_clickhouse_event_DistributedConnectionFailAtAll[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Distributed connections fails occurred"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}`
              please check communications between clickhouse server and host `remote_servers` in `/etc/clickhouse-server/`
              https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-remote-servers
              also, you can check logs:
              ```kubectl logs -n {{ $labels.exported_namespace }} $( echo {{ $labels.hostname }} | cut -d '.' -f 1)-0```

        - alert: ClickHouseRejectedInsert
          expr: increase(chi_clickhouse_event_RejectedInserts[1m]) > 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Rejected INSERT queries occurred"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have INSERT queries that are rejected due to high number of active data parts for partition in a MergeTree, please decrease INSERT frequency
              MergeTreeArchitecture
              https://clickhouse.tech/docs/en/development/architecture/#merge-tree
              system.parts_log
              https://clickhouse.tech/docs/en/operations/system-tables/#system_tables-part-log
              system.merge_tree_settings
              https://clickhouse.tech/docs/en/operations/system-tables/#system-merge_tree_settings


        - alert: ClickHouseDelayedInsertThrottling
          expr: increase(chi_clickhouse_event_DelayedInserts[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Delayed INSERT queries occurred"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have INSERT queries that are throttled due to high number of active data parts for partition in a MergeTree, please decrease INSERT frequency
              https://clickhouse.tech/docs/en/development/architecture/#merge-tree

        - alert: ClickHouseMaxPartCountForPartition
          expr: chi_clickhouse_metric_MaxPartCountForPartition > 100
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Max parts per partition > 100"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have too many parts in one partition.
              Clickhouse MergeTree table engine split each INSERT query to partitions (PARTITION BY expression)
              and add one or more PARTS per INSERT inside each partition, after that background merge process run,
              and when you have too much unmerged parts inside partition,
              SELECT queries performance can significate degrade, so clickhouse try delay or reject INSERT

        - alert: ClickHouseLowInsertedRowsPerQuery
          expr: increase(chi_clickhouse_event_InsertQuery[1m]) > 0 and (increase(chi_clickhouse_event_InsertedRows[1m]) / increase(chi_clickhouse_event_InsertQuery[1m]) <= 1000)
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "please increase inserted rows per INSERT query"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have insert speed {{ with printf "increase(chi_clickhouse_event_InsertedRows{hostname='%s',exported_namespace='%s'}[1m]) / increase(chi_clickhouse_event_InsertQuery{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }} rows per query {{ end }}
              https://clickhouse.tech/docs/en/introduction/performance/#performance-when-inserting-data
              Clickhouse team recommends inserting data in packets of at least 1000 rows or no more than a single request per second.

              Please use Buffer table
              https://clickhouse.tech/docs/en/operations/table_engines/buffer/&#13;
              or
              https://github.com/nikepan/clickhouse-bulk
              or
              https://github.com/VKCOM/kittenhouse

        - alert: ClickHouseLongestRunningQuery
          expr: chi_clickhouse_metric_LongestRunningQuery > 600
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Long running queries occurred"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have queries that running more than 10 minutes
              try look to system.processes
              https://clickhouse.tech/docs/en/operations/system-tables/#system_tables-processes

        - alert: ClickHouseQueryPreempted
          expr: chi_clickhouse_metric_QueryPreempted > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Preempted queries occurred"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have queries that are stopped and waiting due to 'priority' setting.
              try look to system.processes
              https://clickhouse.tech/docs/en/operations/system-tables/#system_tables-processes

        - alert: ClickHouseReadonlyReplica
          expr: chi_clickhouse_metric_ReadonlyReplica > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "ReadOnly replica occurred"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have ReplicatedMergeTree tables that are currently in readonly state due to re-initialization after ZooKeeper session loss or due to startup without ZooKeeper configured.
              Please check following things:
              - kubenetes nodes have free enough RAM and Disk via `kubectl top node`
              - status of clickhouse-server pods `kubectl describe -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1)`
              - connection between clickhouse-server pods via kubernetes services
              - connection between clickhouse-server pods and zookeeper
              - status of PersistentVolumeClaims for pods
              Also read documentation:
              https://clickhouse.tech/docs/en/operations/table_engines/replication/#recovery-after-failures

        - alert: ClickHouseReplicasMaxAbsoluteDelay
          expr: chi_clickhouse_metric_ReplicasMaxAbsoluteDelay > 300
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Replication Lag more 300s occurred"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have replication lag.
              When replica have too much lag, it can be skipped from Distributed SELECT Queries without errors and you will have wrong query results.
              Check free disk space disks and network connection between clickhouse pod and zookeeper on monitored clickhouse-server pods

              Also read documentation:
              - https://clickhouse.tech/docs/en/operations/table_engines/replication/#recovery-after-failures
              - https://clickhouse.tech/docs/en/operations/settings/settings/#settings-max_replica_delay_for_distributed_queries

        - alert: ClickHouseTooManyConnections
          expr: chi_clickhouse_metric_HTTPConnection + chi_clickhouse_metric_TCPConnection + chi_clickhouse_metric_MySQLConnection > 100
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Total connections > 100"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have many open connections.
              The ClickHouse is adapted to run not a very large number of parallel SQL requests, not every HTTP/TCP(Native)/MySQL protocol connection means a running SQL request, but a large number of open connections can cause a spike in sudden SQL requests, resulting in performance degradation.

              Also read documentation:
              - https://clickhouse.tech/docs/en/operations/server_settings/settings/#max-concurrent-queries


        - alert: ClickHouseTooMuchRunningQueries
          expr: chi_clickhouse_metric_Query > 80
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Too much running queries"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` have {{ with printf "chi_clickhouse_metric_Query{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }}{{ end }} running queries.
              Please analyze your workload.
              Each concurrent SELECT query use memory in JOINs use CPU for running aggregation function and can read lot of data from disk when scan parts in partitions and utilize disk I/O.
              Each concurrent INSERT query, allocate around 1MB per each column in an inserted table and utilize disk I/O.

              Look at following documentation parts:
              - https://clickhouse.tech/docs/en/operations/settings/query_complexity/
              - https://clickhouse.tech/docs/en/operations/quotas/
              - https://clickhouse.tech/docs/en/operations/server_settings/settings/#max-concurrent-queries
              - https://clickhouse.tech/docs/en/operations/system-tables/#system_tables-query_log

        - alert: ClickHouseSystemSettingsChanged
          expr: delta(chi_clickhouse_metric_ChangedSettingsHash[5m]) != 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "`system.settings` changed"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}`

        - alert: ClickHouseVersionChanged
          expr: delta(chi_clickhouse_metric_VersionInteger[5m]) != 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "ClickHouse version changed"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}`

        - alert: ZooKeeperHardwareExceptions
          expr: delta(chi_clickhouse_event_ZooKeeperHardwareExceptions[3m]) != 0 > 1
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "ZooKeeperHardwareExceptions > 1"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}`
              We have unexpected Network errors and similar with communitation with Zookeeper.
              Clickhouse should reinitialize ZooKeeper session in case of these errors.

        - alert: ZooKeeperSession
          expr: chi_clickhouse_metric_ZooKeeperSession > 1
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "ZooKeeperSession > 1"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}`
              Number of sessions (connections) to ZooKeeper shall be no more than one, because using more than one
              connection to ZooKeeper may lead to bugs due to lack of linearizability (stale reads)
              that ZooKeeper consistency model allows.

        - alert: ClickHouseDiskUsage
          expr: (chi_clickhouse_metric_DiskDataBytes / (chi_clickhouse_metric_DiskFreeBytes + chi_clickhouse_metric_DiskDataBytes)) > 0.8
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Disk Usage in `&lt;yandex&gt;&lt;path&gt;` > 80%"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}`
              data size: {{ with printf "chi_clickhouse_metric_DiskDataBytes{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | humanize1024 }}B {{ end }}
              disk free {{ with printf "chi_clickhouse_metric_DiskFreeBytes{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | humanize1024 }}B {{ end }}
              currently k8s doesn't support resize of Persistent Volumes, but you can try add another volume to existing pod with restart pod
              please read documentation:
              - https://github.com/Altinity/clickhouse-operator/blob/master/docs/storage.md
              - https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-multiple-volumes
              - https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl

# not well tested alerts which can't be triggered on e2e tests
        - alert: ClickHouseReplicatedPartChecksFailed
          expr: increase(chi_clickhouse_event_ReplicatedPartChecksFailed[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased ReplicatedPartCheckFailed"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` increase ReplicatedPartCheckFailed in `system.events` table.
              Please check following things:
              - logs on clickhouse-server pods `kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) cat /var/log/clickhouse-server/*.err.log | less`

        - alert: ClickHouseReplicatedPartFailedFetches
          expr: increase(chi_clickhouse_event_ReplicatedPartFailedFetches[1m]) > 0 and increase(chi_clickhouse_event_ReplicatedPartFetches[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased ReplicatedPartFailedFetches"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` increase ReplicatedPartFailedFetches in `system.events` table.
              It mean server was failed to download data part from replica of a ReplicatedMergeTree table.
              Please check following things:
              - logs on clickhouse-server pods `kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) cat /var/log/clickhouse-server/*.err.log | less`
              - connections between clickhouse-server pod and his replicas (see remote_server section in /etc/clickhouse-server/)

        - alert: ClickHouseReplicatedDataLoss
          expr: increase(chi_clickhouse_event_ReplicatedDataLoss[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased ReplicatedDataLoss"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` increase ReplicatedDataLoss in `system.events` table.
              It mean data part that server wanted doesn't exist on any replica (even on replicas that are offline right now).
              That data parts are definitely lost. This is normal due to asynchronous replication (if quorum inserts were not enabled),
              when the replica on which the data part was written was failed and when it became online after fail
              it doesn't contain that data part.

              Please check following things:
              - logs on clickhouse-server pods `kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) cat /var/log/clickhouse-server/*.err.log | less`

        - alert: ClickHouseStorageBufferErrorOnFlush
          expr: increase(chi_clickhouse_event_StorageBufferErrorOnFlush[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased StorageBufferErrorOnFlush"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` increase StorageBufferErrorOnFlush in `system.events` table.
              It mean something went wrong when clickhouse-server try to flush memory buffers to disk.

              Please check following things:
              - logs on clickhouse-server pods `kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) cat /var/log/clickhouse-server/*.err.log | less`
              - check disks free space and hardware failures

        - alert: ClickHouseDataAfterMergeDiffersFromReplica
          expr: increase(chi_clickhouse_event_DataAfterMergeDiffersFromReplica[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased DataAfterMergeDiffersFromReplica"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` increase DataAfterMergeDiffersFromReplica in `system.events` table.
              It mean Data after merge is not byte-identical to data on another replicas.
              There could be several reasons:
              - Using newer version of compression library after server update.
              - Using another compression method.
              - Non-deterministic compression algorithm (highly unlikely).
              - Non-deterministic merge algorithm due to logical error in code.
              - Data corruption in memory due to bug in code.
              - Data corruption in memory due to hardware issue.
              - Manual modification of source data after server startup.
              - Manual modification of checksums stored in ZooKeeper.
              - Part format related settings like 'enable_mixed_granularity_parts' are different on different replicas.

              Server will download merged part from replica to force byte-identical result.

        - alert: ClickHouseDistributedSyncInsertionTimeoutExceeded
          expr: increase(chi_clickhouse_event_DistributedSyncInsertionTimeoutExceeded[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased DistributedSyncInsertionTimeoutExceeded"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` increase DistributedSyncInsertionTimeoutExceeded in `system.events` table.
              It mean Synchronous distributed insert timeout exceeded.
              Please check documentation https://clickhouse.tech/docs/en/operations/settings/settings/#insert_distributed_sync
              And check connection between `{{ $labels.hostname }}` and all nodes in shards from remote_servers config section

        - alert: ClickHouseFileDescriptorBufferReadOrWriteFailed
          expr: increase(chi_clickhouse_event_ReadBufferFromFileDescriptorReadFailed[1m]) > 0 or increase(chi_clickhouse_event_WriteBufferFromFileDescriptorWriteFailed[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased ReadBufferFromFileDescriptorReadFailed or WriteBufferFromFileDescriptorWriteFailed"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` increase ReadBufferFromFileDescriptorReadFailed or ReadBufferFromFileDescriptorReadFailed in `system.events` table.
              It mean the read (read/pread) or writes (write/pwrite) to a file descriptor. Does not include sockets.
              System can't read or write to some files.
              - logs on clickhouse-server pods `kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) cat /var/log/clickhouse-server/*.err.log | grep -E "Cannot write to file|Cannot read from file"`

        - alert: ClickHouseSlowRead
          expr: increase(chi_clickhouse_event_SlowRead[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased SlowRead"
            description: |-
              `{{ $labels.hostname }}` in namespace `{{ $labels.exported_namespace }}` increase SlowRead in `system.events` table.
              It mean reads from a files that were slow. This indicate system overload. Thresholds are controlled by `SELECT * FROM system.settings WHERE name LIKE 'read_backoff_%'`.
              System will reduce the number of threads which used for processing queries.
              Check you disks utilization and hardware failures.
